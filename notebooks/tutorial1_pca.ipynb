{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Principal Component Analysis (PCA)\n",
    "\n",
    "**CSCI 8945: Advanced Representation Learning**  \n",
    "**Fall 2025**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/csci8945-advanced-representation-learning/blob/main/notebooks/tutorial1_pca.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "1. **Understand the mathematical foundations** of Principal Component Analysis (PCA)\n",
    "2. **Implement PCA from scratch** using NumPy\n",
    "3. **Apply PCA** to real datasets for dimensionality reduction\n",
    "4. **Interpret PCA results** and visualize principal components\n",
    "5. **Compare custom implementation** with sklearn's PCA\n",
    "6. **Understand the limitations** and assumptions of PCA\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Dimensionality Reduction](#1-introduction)\n",
    "2. [Mathematical Foundations of PCA](#2-mathematical-foundations)\n",
    "3. [Implementation from Scratch](#3-implementation-from-scratch)\n",
    "4. [Application to Real Data](#4-application-to-real-data)\n",
    "5. [Comparison with sklearn](#5-comparison-with-sklearn)\n",
    "6. [Visualization and Interpretation](#6-visualization-and-interpretation)\n",
    "7. [Limitations and Extensions](#7-limitations-and-extensions)\n",
    "8. [Exercises](#8-exercises)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction to Dimensionality Reduction {#1-introduction}\n",
    "\n",
    "**Dimensionality reduction** is a fundamental problem in machine learning and data science. Real-world datasets often contain hundreds or thousands of features, but not all of these features are equally important for understanding the underlying structure of the data.\n",
    "\n",
    "### Why do we need dimensionality reduction?\n",
    "\n",
    "1. **Curse of dimensionality**: As the number of dimensions increases, data becomes sparse\n",
    "2. **Computational efficiency**: Fewer dimensions mean faster algorithms\n",
    "3. **Visualization**: Humans can only visualize 2D or 3D data effectively\n",
    "4. **Noise reduction**: Remove irrelevant features that might be noise\n",
    "5. **Storage**: Reduce memory and storage requirements\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is one of the most widely used techniques for dimensionality reduction. It finds the directions (principal components) along which the data varies the most and projects the data onto these directions.\n",
    "\n",
    "**Key Idea**: Find a lower-dimensional representation that preserves as much variance as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical Foundations of PCA {#2-mathematical-foundations}\n",
    "\n",
    "### 2.1 Problem Setup\n",
    "\n",
    "Let's say we have a dataset $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ where:\n",
    "- $n$ is the number of samples\n",
    "- $d$ is the number of features/dimensions\n",
    "- Each row $\\mathbf{x}_i$ represents one data sample\n",
    "\n",
    "**Goal**: Find a lower-dimensional representation $\\mathbf{Z} \\in \\mathbb{R}^{n \\times k}$ where $k < d$.\n",
    "\n",
    "### 2.2 Mathematical Formulation\n",
    "\n",
    "#### Step 1: Center the data\n",
    "First, we center the data by subtracting the mean:\n",
    "$$\\tilde{\\mathbf{X}} = \\mathbf{X} - \\boldsymbol{\\mu}$$\n",
    "where $\\boldsymbol{\\mu} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbf{x}_i$ is the sample mean.\n",
    "\n",
    "#### Step 2: Compute the covariance matrix\n",
    "The sample covariance matrix is:\n",
    "$$\\mathbf{C} = \\frac{1}{n-1}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$$\n",
    "\n",
    "#### Step 3: Eigenvalue decomposition\n",
    "Find the eigenvalues $\\lambda_i$ and eigenvectors $\\mathbf{v}_i$ of the covariance matrix:\n",
    "$$\\mathbf{C}\\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$$\n",
    "\n",
    "The eigenvectors are the **principal components** and the eigenvalues represent the **variance** explained by each component.\n",
    "\n",
    "#### Step 4: Select top-k components\n",
    "Sort eigenvalues in descending order: $\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_d$\n",
    "\n",
    "Select the first $k$ eigenvectors to form the projection matrix:\n",
    "$$\\mathbf{W} = [\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_k] \\in \\mathbb{R}^{d \\times k}$$\n",
    "\n",
    "#### Step 5: Project the data\n",
    "The lower-dimensional representation is:\n",
    "$$\\mathbf{Z} = \\tilde{\\mathbf{X}}\\mathbf{W}$$\n",
    "\n",
    "### 2.3 Key Properties\n",
    "\n",
    "1. **Variance maximization**: The first principal component maximizes the variance of the projected data\n",
    "2. **Orthogonality**: Principal components are orthogonal to each other\n",
    "3. **Reconstruction**: Original data can be approximately reconstructed as $\\hat{\\mathbf{X}} = \\mathbf{Z}\\mathbf{W}^T + \\boldsymbol{\\mu}$\n",
    "4. **Variance explained**: The ratio $\\frac{\\lambda_i}{\\sum_j \\lambda_j}$ gives the proportion of variance explained by the $i$-th component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris, load_digits, make_blobs\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation from Scratch {#3-implementation-from-scratch}\n",
    "\n",
    "Let's implement PCA from scratch following the mathematical steps we outlined above. This will help us understand exactly what's happening under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis implemented from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_components : int, optional (default=None)\n",
    "        Number of components to keep. If None, keep all components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components_ = None\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "        self.mean_ = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA on the data X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \"\"\"\n",
    "        # Convert to numpy array if needed\n",
    "        X = np.array(X)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Step 1: Center the data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # Step 2: Compute covariance matrix\n",
    "        # Note: Using (n-1) for unbiased estimator\n",
    "        cov_matrix = np.cov(X_centered.T)\n",
    "        \n",
    "        # Step 3: Eigenvalue decomposition\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        # Step 4: Sort eigenvalues and eigenvectors in descending order\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Step 5: Select number of components\n",
    "        if self.n_components is None:\n",
    "            self.n_components = n_features\n",
    "        \n",
    "        self.components_ = eigenvectors[:, :self.n_components].T\n",
    "        self.explained_variance_ = eigenvalues[:self.n_components]\n",
    "        self.explained_variance_ratio_ = self.explained_variance_ / np.sum(eigenvalues)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply dimensionality reduction to X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : array, shape (n_samples, n_components)\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        X_centered = X - self.mean_\n",
    "        return np.dot(X_centered, self.components_.T)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit PCA and transform the data.\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"\n",
    "        Transform data back to original space.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_transformed : array-like, shape (n_samples, n_components)\n",
    "            Transformed data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        X_reconstructed : array, shape (n_samples, n_features)\n",
    "            Reconstructed data\n",
    "        \"\"\"\n",
    "        return np.dot(X_transformed, self.components_) + self.mean_\n",
    "    \n",
    "    def get_cumulative_variance_ratio(self):\n",
    "        \"\"\"\n",
    "        Get cumulative explained variance ratio.\n",
    "        \"\"\"\n",
    "        return np.cumsum(self.explained_variance_ratio_)\n",
    "\n",
    "# Test our implementation with a simple example\n",
    "print(\"PCA class implemented successfully!\")\n",
    "\n",
    "# Create a simple 2D dataset for testing\n",
    "np.random.seed(42)\n",
    "X_simple = np.random.randn(100, 2)\n",
    "X_simple[:, 1] = X_simple[:, 0] + 0.5 * np.random.randn(100)  # Create correlation\n",
    "\n",
    "# Fit PCA\n",
    "pca_custom = PCA(n_components=2)\n",
    "X_transformed = pca_custom.fit_transform(X_simple)\n",
    "\n",
    "print(f\"Original data shape: {X_simple.shape}\")\n",
    "print(f\"Transformed data shape: {X_transformed.shape}\")\n",
    "print(f\"Explained variance ratio: {pca_custom.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {pca_custom.get_cumulative_variance_ratio()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application to Real Data {#4-application-to-real-data}\n",
    "\n",
    "Let's apply our PCA implementation to the famous Iris dataset and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data  # 4 features: sepal length, sepal width, petal length, petal width\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Iris Dataset Information:\")\n",
    "print(f\"Shape: {X_iris.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Classes: {target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y_iris)}\")\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "print(f\"\\nOriginal data statistics:\")\n",
    "print(f\"Mean: {np.mean(X_iris, axis=0)}\")\n",
    "print(f\"Std:  {np.std(X_iris, axis=0)}\")\n",
    "\n",
    "print(f\"\\nScaled data statistics:\")\n",
    "print(f\"Mean: {np.mean(X_iris_scaled, axis=0)}\")\n",
    "print(f\"Std:  {np.std(X_iris_scaled, axis=0)}\")\n",
    "\n",
    "# Apply our PCA implementation\n",
    "pca_iris = PCA(n_components=2)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"Explained variance ratio: {pca_iris.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative explained variance: {pca_iris.get_cumulative_variance_ratio()}\")\n",
    "print(f\"Total variance explained by 2 components: {np.sum(pca_iris.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "# Visualize the results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original data (first two features)\n",
    "scatter1 = axes[0].scatter(X_iris_scaled[:, 0], X_iris_scaled[:, 1], \n",
    "                          c=y_iris, cmap='viridis', alpha=0.7)\n",
    "axes[0].set_xlabel(feature_names[0])\n",
    "axes[0].set_ylabel(feature_names[1])\n",
    "axes[0].set_title('Original Data (First 2 Features)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA transformed data\n",
    "scatter2 = axes[1].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], \n",
    "                          c=y_iris, cmap='viridis', alpha=0.7)\n",
    "axes[1].set_xlabel('First Principal Component')\n",
    "axes[1].set_ylabel('Second Principal Component')\n",
    "axes[1].set_title('PCA Transformed Data')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(scatter2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show principal components\n",
    "print(f\"\\nPrincipal Components:\")\n",
    "for i, component in enumerate(pca_iris.components_):\n",
    "    print(f\"PC{i+1}: {component}\")\n",
    "    print(f\"      Features contribution: {dict(zip(feature_names, component))}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with sklearn {#5-comparison-with-sklearn}\n",
    "\n",
    "Let's compare our implementation with sklearn's PCA to verify our results are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sklearn's PCA\n",
    "pca_sklearn = SklearnPCA(n_components=2)\n",
    "X_iris_sklearn = pca_sklearn.fit_transform(X_iris_scaled)\n",
    "\n",
    "# Compare results\n",
    "print(\"Comparison between our PCA and sklearn's PCA:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nExplained Variance Ratio:\")\n",
    "print(f\"Our PCA:     {pca_iris.explained_variance_ratio_}\")\n",
    "print(f\"Sklearn PCA: {pca_sklearn.explained_variance_ratio_}\")\n",
    "print(f\"Difference:  {np.abs(pca_iris.explained_variance_ratio_ - pca_sklearn.explained_variance_ratio_)}\")\n",
    "\n",
    "print(f\"\\nExplained Variance:\")\n",
    "print(f\"Our PCA:     {pca_iris.explained_variance_}\")\n",
    "print(f\"Sklearn PCA: {pca_sklearn.explained_variance_}\")\n",
    "print(f\"Difference:  {np.abs(pca_iris.explained_variance_ - pca_sklearn.explained_variance_)}\")\n",
    "\n",
    "print(f\"\\nPrincipal Components (absolute values):\")\n",
    "print(f\"Our PCA PC1:     {np.abs(pca_iris.components_[0])}\")\n",
    "print(f\"Sklearn PC1:     {np.abs(pca_sklearn.components_[0])}\")\n",
    "print(f\"Difference PC1:  {np.abs(np.abs(pca_iris.components_[0]) - np.abs(pca_sklearn.components_[0]))}\")\n",
    "\n",
    "print(f\"\\nOur PCA PC2:     {np.abs(pca_iris.components_[1])}\")\n",
    "print(f\"Sklearn PC2:     {np.abs(pca_sklearn.components_[1])}\")\n",
    "print(f\"Difference PC2:  {np.abs(np.abs(pca_iris.components_[1]) - np.abs(pca_sklearn.components_[1]))}\")\n",
    "\n",
    "# Note: Signs of eigenvectors can be flipped, so we compare absolute values\n",
    "# The direction doesn't matter for PCA, only the subspace spanned by the eigenvectors\n",
    "\n",
    "# Compare transformed data (accounting for potential sign flips)\n",
    "correlation_pc1 = np.corrcoef(X_iris_pca[:, 0], X_iris_sklearn[:, 0])[0, 1]\n",
    "correlation_pc2 = np.corrcoef(X_iris_pca[:, 1], X_iris_sklearn[:, 1])[0, 1]\n",
    "\n",
    "print(f\"\\nCorrelation between transformed data:\")\n",
    "print(f\"PC1 correlation: {np.abs(correlation_pc1):.6f}\")\n",
    "print(f\"PC2 correlation: {np.abs(correlation_pc2):.6f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Our PCA\n",
    "scatter1 = axes[0].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], \n",
    "                          c=y_iris, cmap='viridis', alpha=0.7)\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component')\n",
    "axes[0].set_title('Our PCA Implementation')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sklearn PCA\n",
    "scatter2 = axes[1].scatter(X_iris_sklearn[:, 0], X_iris_sklearn[:, 1], \n",
    "                          c=y_iris, cmap='viridis', alpha=0.7)\n",
    "axes[1].set_xlabel('First Principal Component')\n",
    "axes[1].set_ylabel('Second Principal Component')\n",
    "axes[1].set_title('Sklearn PCA')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Difference\n",
    "scatter3 = axes[2].scatter(X_iris_pca[:, 0] - X_iris_sklearn[:, 0], \n",
    "                          X_iris_pca[:, 1] - X_iris_sklearn[:, 1], \n",
    "                          c=y_iris, cmap='viridis', alpha=0.7)\n",
    "axes[2].set_xlabel('PC1 Difference')\n",
    "axes[2].set_ylabel('PC2 Difference')\n",
    "axes[2].set_title('Difference (Our - Sklearn)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reconstruction test\n",
    "X_reconstructed_ours = pca_iris.inverse_transform(X_iris_pca)\n",
    "X_reconstructed_sklearn = pca_sklearn.inverse_transform(X_iris_sklearn)\n",
    "\n",
    "reconstruction_error_ours = np.mean((X_iris_scaled - X_reconstructed_ours) ** 2)\n",
    "reconstruction_error_sklearn = np.mean((X_iris_scaled - X_reconstructed_sklearn) ** 2)\n",
    "\n",
    "print(f\"\\nReconstruction Error (MSE):\")\n",
    "print(f\"Our PCA:     {reconstruction_error_ours:.8f}\")\n",
    "print(f\"Sklearn PCA: {reconstruction_error_sklearn:.8f}\")\n",
    "print(f\"Difference:  {np.abs(reconstruction_error_ours - reconstruction_error_sklearn):.8f}\")\n",
    "\n",
    "print(\"\\n✅ Our implementation matches sklearn's PCA!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization and Interpretation {#6-visualization-and-interpretation}\n",
    "\n",
    "Let's explore different ways to visualize and interpret PCA results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization of PCA results\n",
    "\n",
    "# 1. Scree Plot - showing explained variance by each component\n",
    "pca_all = PCA()  # Fit PCA with all components\n",
    "pca_all.fit(X_iris_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Scree plot\n",
    "axes[0, 0].plot(range(1, len(pca_all.explained_variance_ratio_) + 1), \n",
    "                pca_all.explained_variance_ratio_, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_xlabel('Principal Component')\n",
    "axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0, 0].set_title('Scree Plot')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_xticks(range(1, len(pca_all.explained_variance_ratio_) + 1))\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_var = pca_all.get_cumulative_variance_ratio()\n",
    "axes[0, 1].plot(range(1, len(cumulative_var) + 1), cumulative_var, 'ro-', linewidth=2, markersize=8)\n",
    "axes[0, 1].axhline(y=0.95, color='k', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "axes[0, 1].axhline(y=0.90, color='gray', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "axes[0, 1].set_xlabel('Number of Components')\n",
    "axes[0, 1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[0, 1].set_title('Cumulative Explained Variance')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xticks(range(1, len(cumulative_var) + 1))\n",
    "\n",
    "# Component loadings (biplot style)\n",
    "feature_names_short = ['SL', 'SW', 'PL', 'PW']  # Shortened names\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "\n",
    "for i, (feature, color) in enumerate(zip(feature_names_short, colors)):\n",
    "    axes[1, 0].arrow(0, 0, pca_iris.components_[0, i]*3, pca_iris.components_[1, i]*3,\n",
    "                     head_width=0.1, head_length=0.1, fc=color, ec=color, linewidth=2, label=feature)\n",
    "    axes[1, 0].text(pca_iris.components_[0, i]*3.2, pca_iris.components_[1, i]*3.2, \n",
    "                    feature, fontsize=10, color=color, weight='bold')\n",
    "\n",
    "# Add data points\n",
    "scatter = axes[1, 0].scatter(X_iris_pca[:, 0], X_iris_pca[:, 1], \n",
    "                            c=y_iris, cmap='viridis', alpha=0.6, s=50)\n",
    "axes[1, 0].set_xlabel('First Principal Component')\n",
    "axes[1, 0].set_ylabel('Second Principal Component')\n",
    "axes[1, 0].set_title('PCA Biplot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Component contributions heatmap\n",
    "component_matrix = np.abs(pca_iris.components_)\n",
    "im = axes[1, 1].imshow(component_matrix, cmap='Blues', aspect='auto')\n",
    "axes[1, 1].set_xticks(range(len(feature_names)))\n",
    "axes[1, 1].set_xticklabels(feature_names_short, rotation=45)\n",
    "axes[1, 1].set_yticks(range(len(pca_iris.components_)))\n",
    "axes[1, 1].set_yticklabels([f'PC{i+1}' for i in range(len(pca_iris.components_))])\n",
    "axes[1, 1].set_title('Component Loadings Heatmap')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(component_matrix.shape[0]):\n",
    "    for j in range(component_matrix.shape[1]):\n",
    "        text = axes[1, 1].text(j, i, f'{component_matrix[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "print(\"Interpretation of PCA Results:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"• PC1 explains {pca_iris.explained_variance_ratio_[0]:.1%} of the variance\")\n",
    "print(f\"• PC2 explains {pca_iris.explained_variance_ratio_[1]:.1%} of the variance\")\n",
    "print(f\"• Together, they explain {np.sum(pca_iris.explained_variance_ratio_):.1%} of the total variance\")\n",
    "\n",
    "print(f\"\\nPC1 is most influenced by:\")\n",
    "pc1_influences = [(feature_names[i], abs(pca_iris.components_[0, i])) for i in range(len(feature_names))]\n",
    "pc1_influences.sort(key=lambda x: x[1], reverse=True)\n",
    "for feature, influence in pc1_influences:\n",
    "    print(f\"  • {feature}: {influence:.3f}\")\n",
    "\n",
    "print(f\"\\nPC2 is most influenced by:\")\n",
    "pc2_influences = [(feature_names[i], abs(pca_iris.components_[1, i])) for i in range(len(feature_names))]\n",
    "pc2_influences.sort(key=lambda x: x[1], reverse=True)\n",
    "for feature, influence in pc2_influences:\n",
    "    print(f\"  • {feature}: {influence:.3f}\")\n",
    "\n",
    "# Show how many components needed for different variance thresholds\n",
    "for threshold in [0.8, 0.9, 0.95, 0.99]:\n",
    "    n_components_needed = np.argmax(cumulative_var >= threshold) + 1\n",
    "    print(f\"\\nTo explain {threshold:.0%} of variance, you need {n_components_needed} component(s)\")\n",
    "    print(f\"Actual variance explained: {cumulative_var[n_components_needed-1]:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Limitations and Extensions {#7-limitations-and-extensions}\n",
    "\n",
    "### 7.1 Limitations of PCA\n",
    "\n",
    "1. **Linear assumption**: PCA only captures linear relationships\n",
    "2. **Interpretation**: Principal components may not have clear physical meaning\n",
    "3. **Scaling sensitivity**: Results depend heavily on feature scaling\n",
    "4. **Outlier sensitivity**: PCA is sensitive to outliers\n",
    "5. **Global technique**: Uses all data points to find components\n",
    "\n",
    "### 7.2 When PCA works well\n",
    "\n",
    "- Features are correlated\n",
    "- Linear relationships dominate\n",
    "- Data is roughly Gaussian\n",
    "- You need dimensionality reduction\n",
    "- Interpretability is not critical\n",
    "\n",
    "### 7.3 When PCA might not work well\n",
    "\n",
    "- Strong non-linear relationships\n",
    "- Categorical or binary features\n",
    "- Very sparse data\n",
    "- When you need interpretable features\n",
    "\n",
    "### 7.4 Extensions and Alternatives\n",
    "\n",
    "1. **Kernel PCA**: Handles non-linear relationships\n",
    "2. **Sparse PCA**: For interpretable components\n",
    "3. **Robust PCA**: Less sensitive to outliers\n",
    "4. **Incremental PCA**: For large datasets\n",
    "5. **t-SNE**: For visualization of non-linear structures\n",
    "6. **UMAP**: Another non-linear dimensionality reduction technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exercises {#8-exercises}\n",
    "\n",
    "Now it's time to practice! Try these exercises to solidify your understanding of PCA.\n",
    "\n",
    "### Exercise 1: PCA on Digits Dataset (Easy)\n",
    "Load the digits dataset from sklearn and apply PCA to reduce it from 64 dimensions to 2. Visualize the results and calculate how much variance is explained.\n",
    "\n",
    "### Exercise 2: Effect of Scaling (Medium)\n",
    "Compare PCA results on the Iris dataset with and without standardization. What differences do you observe? Why does this happen?\n",
    "\n",
    "### Exercise 3: Choosing Number of Components (Medium)\n",
    "Using the digits dataset, create a plot showing reconstruction error vs. number of components. Find the \"elbow point\" where adding more components doesn't significantly reduce the error.\n",
    "\n",
    "### Exercise 4: PCA for Image Compression (Hard)\n",
    "Implement a simple image compression algorithm using PCA. Load a grayscale image, apply PCA, and reconstruct the image using different numbers of components. Show the trade-off between compression ratio and image quality.\n",
    "\n",
    "### Exercise 5: Custom Dataset (Hard)\n",
    "Create a synthetic 3D dataset where the data lies approximately on a 2D plane. Add some noise and apply PCA. Visualize the original data and the principal components. Does PCA correctly identify the underlying 2D structure?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully completed the PCA tutorial. Here's what you've learned:\n",
    "\n",
    "✅ **Mathematical foundations** of PCA  \n",
    "✅ **Implementation from scratch** using NumPy  \n",
    "✅ **Application** to real datasets  \n",
    "✅ **Comparison** with sklearn implementation  \n",
    "✅ **Visualization and interpretation** techniques  \n",
    "✅ **Limitations** and when to use PCA\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **PCA finds directions of maximum variance** in the data\n",
    "2. **Principal components are orthogonal** to each other\n",
    "3. **Standardization is crucial** for meaningful results\n",
    "4. **PCA is a linear technique** - it won't capture non-linear relationships\n",
    "5. **Dimensionality reduction** involves a trade-off between simplicity and information loss\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next tutorial, we'll explore **Kernel PCA** and other non-linear dimensionality reduction techniques that can capture more complex patterns in data.\n",
    "\n",
    "---\n",
    "\n",
    "**Happy learning! 🎓**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
